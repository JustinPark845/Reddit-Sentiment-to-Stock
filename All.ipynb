{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOC+395mzJkhgMOw2VFaO2P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JustinPark845/Natural-Language-Processing-Final-Project/blob/main/All.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCwrCsWpnDWs"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SPLIT DATA**"
      ],
      "metadata": {
        "id": "zYFCl6zYozeq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ast\n",
        "f=open(\"/content/drive/MyDrive/nlp_project/text_to_label.txt\", \"r\")\n",
        "text_to_label = ast.literal_eval(f.read())\n",
        "f.close()"
      ],
      "metadata": {
        "id": "qMjnOb_Ho26N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_data_train = []\n",
        "text_label_train = []\n",
        "\n",
        "text_data_test = []\n",
        "text_label_test = []\n",
        "\n",
        "for diction in text_to_label[:round(len(text_to_label)*.80)]:\n",
        "  text_data_train.append(diction[\"text\"])\n",
        "  text_label_train.append(diction[\"label\"])\n",
        "for diction in text_to_label[round(len(text_to_label)*.80):]:\n",
        "  text_data_test.append(diction[\"text\"])\n",
        "  text_label_test.append(diction[\"label\"])"
      ],
      "metadata": {
        "id": "3cEJmJUbo5EF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create csv\n",
        "import csv  \n",
        "\n",
        "header = ['text','label']\n",
        "\n",
        "with open('/content/drive/MyDrive/nlp_project/text_to_label_train.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    for i in text_to_label[:round(len(text_to_label)*.80)]:\n",
        "      if i[\"label\"] < 0:\n",
        "        writer.writerow([i[\"text\"],0])\n",
        "      else:\n",
        "        writer.writerow([i[\"text\"],i[\"label\"]])\n",
        "\n",
        "with open('/content/drive/MyDrive/nlp_project/text_to_label_test.csv', 'w', encoding='UTF8') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(header)\n",
        "    for i in text_to_label[round(len(text_to_label)*.80):]:\n",
        "      if i[\"label\"] < 0:\n",
        "        writer.writerow([i[\"text\"],0])\n",
        "      else:\n",
        "        writer.writerow([i[\"text\"],i[\"label\"]])"
      ],
      "metadata": {
        "id": "po-GM2XLvdSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**CLASSIFIERS**"
      ],
      "metadata": {
        "id": "WJdxRqFyn_sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(text_data_train + text_data_test)"
      ],
      "metadata": {
        "id": "31k3rc6KoH2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
      ],
      "metadata": {
        "id": "KyjEQF_AoIy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# for later testing\n",
        "nptest = X_train_tfidf.toarray()\n",
        "npdata = X_train_tfidf.toarray()[:round(len(text_to_label)*.80)]\n",
        "nptarget = text_label_train"
      ],
      "metadata": {
        "id": "ZYQsCgp0oJtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "\n",
        "# These are the scoring metrics we will consider\n",
        "scoring_metrics = ['accuracy', 'precision', 'recall', 'f1']"
      ],
      "metadata": {
        "id": "P6ahBXiLoK7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(npdata,nptarget)"
      ],
      "metadata": {
        "id": "R4utk2GOpnOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn import tree\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "dtree = tree.DecisionTreeClassifier()\n",
        "lsvc = make_pipeline(StandardScaler(), LinearSVC(random_state=0, tol=1e-5))\n",
        "\n",
        "knn.fit(npdata,nptarget)\n",
        "dtree.fit(npdata,nptarget)\n",
        "lsvc.fit(npdata,nptarget)"
      ],
      "metadata": {
        "id": "_gl8ZNtqprjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**BERT**"
      ],
      "metadata": {
        "id": "GxWj7XyIqw9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "IF8S5sMBMlxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertModel, AutoTokenizer\n",
        "\n",
        "model = BertModel.from_pretrained('/content/drive/MyDrive/nlp_project')"
      ],
      "metadata": {
        "id": "czPVpihf-eu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch \n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "3SkjNBG_MezE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**SENTIMENT**"
      ],
      "metadata": {
        "id": "MiuAT7E-qX04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download all three sentiments\n",
        "!pip install flair\n",
        "!pip install spacy==3.4\n",
        "!pip install spacytextblob\n",
        "!pip install vaderSentiment\n",
        "!pip install pysentiment2"
      ],
      "metadata": {
        "id": "YxPGehDxqcwc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flair\n",
        "import spacy\n",
        "from flair.models import TextClassifier\n",
        "from flair.data import Sentence\n",
        "from spacytextblob.spacytextblob import SpacyTextBlob\n",
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "import pysentiment2 as ps\n",
        "\n",
        "#Load classifier for English\n",
        "flair_sentiment = TextClassifier.load('en-sentiment')\n",
        "spacy_sentiment = spacy.load('en_core_web_sm')\n",
        "spacy_sentiment.add_pipe('spacytextblob')\n",
        "hiv4 = ps.HIV4()\n",
        "\n",
        "#Combined Function to output array of results in order of [Flair, Spacy, Vader]\n",
        "def combined_FSV(n):\n",
        "  result = []\n",
        "  \n",
        "  #Flair\n",
        "  s = flair.data.Sentence(n)\n",
        "  flair_sentiment.predict(s)\n",
        "  total_sentiment = s.labels[0]\n",
        "  assert total_sentiment.value in ['POSITIVE', 'NEGATIVE']\n",
        "  val_f = 1 if total_sentiment.value == 'POSITIVE' else -1\n",
        "  \n",
        "  #Spacy\n",
        "  text = spacy_sentiment(n)\n",
        "  polarity_score = text._.polarity\n",
        "  if polarity_score > 0:\n",
        "    val_s = 1\n",
        "  else:\n",
        "    val_s = -1\n",
        "  \n",
        "  #Vader\n",
        "  vader_sent = SentimentIntensityAnalyzer()\n",
        "  vader_result = vader_sent.polarity_scores(n)\n",
        "  if vader_result['compound'] > 0.05:\n",
        "    val_v = 1\n",
        "  else:\n",
        "    val_v = -1\n",
        "  \n",
        "  #Pysentiment2\n",
        "  tokens = hiv4.tokenize(n)\n",
        "  val_py = -1\n",
        "  score = hiv4.get_score(tokens)\n",
        "  if score['Positive'] >= score['Negative']:\n",
        "    val_py = 1\n",
        "  \n",
        "  result.append(val_f)\n",
        "  result.append(val_s)\n",
        "  result.append(val_v)\n",
        "  result.append(val_py)\n",
        "  return result"
      ],
      "metadata": {
        "id": "j9EuPOuaqffp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**PUT IT TOGETHER**"
      ],
      "metadata": {
        "id": "9qD4o3zsq5Ja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all = []\n",
        "\n",
        "temp = []\n",
        "for i in range(round(len(text_to_label)*.80),len(text_to_label)-1):\n",
        "  # Sentiment\n",
        "  fsv = combined_FSV(text_to_label[i][\"text\"])\n",
        "  temp.extend(fsv)\n",
        "\n",
        "  # Bert\n",
        "  input_ids = torch.tensor(tokenizer.encode(text_to_label[i][\"text\"], add_special_tokens=True, truncation = True)).unsqueeze(0)  # Batch size 1\n",
        "  labels = torch.tensor([1]).unsqueeze(0)\n",
        "  outputs = model(input_ids,labels)\n",
        "  loss, logits = outputs[:2]\n",
        "  predictions = np.argmax(logits.detach().numpy(), axis=-1)\n",
        "  temp.append(predictions[0])\n",
        "\n",
        "  # Classifiers\n",
        "  classifiers = []\n",
        "  classifiers.append(gnb.predict([nptest[i]])[0])\n",
        "  classifiers.append(knn.predict([nptest[i]])[0])\n",
        "  classifiers.append(dtree.predict([nptest[i]])[0])\n",
        "  classifiers.append(lsvc.predict([nptest[i]])[0])\n",
        "  temp.extend(classifiers)\n",
        "  \n",
        "  all.append((temp,text_to_label[i][\"label\"]))\n",
        "  temp = []"
      ],
      "metadata": {
        "id": "KfMFYev_ploF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f=open(\"/content/drive/MyDrive/nlp_project/all.txt\", \"w\")\n",
        "f.write(str(all))\n",
        "f.close()"
      ],
      "metadata": {
        "id": "YuG2sW3FRk7D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KJ9iN-KnXvb6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}